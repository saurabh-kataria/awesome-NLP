# awesome-NLP
Interesting NLP resources

# Representations
1. [Deep Contextualized Word Representations](https://aclanthology.org/N18-1202) (Peters et al., NAACL 2018) - ELMO
2. [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805) (Devlin et al., NAACL-HLT 2019) - BERT
3. [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781) (Mikolov et al., ICLR 2013 ?) - word2vec
4. CoVe

# Benchmark datasets
1. **Stanford Question Answering Dataset (SQuAD)** for QA - Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP.
2. **Stanford Natural Language Inference (SNLI) corpus** - Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.
3. **OntoNotes benchmark** - Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj¨orkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using ontonotes. In CoNLL.
4. **CoNLL 2003 NER task** - Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoNLL.
5. **Stanford Sentiment Treebank (SST-5)** - Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP.

# Models
1. ESIM sequence model - Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In ACL.

# Background
1. John D. Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In ICML.

# Libraries
1. NLTK (https://www.nltk.org/)
